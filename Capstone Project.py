# -*- coding: utf-8 -*-
"""Gloria's Capstone Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m4aBuMV9iglIh1KX7fRPFGb9h2Ji-RHo
"""



"""**CAPSTONE PROJECT: DATA SCIENCE IN RESTAURANT ANALYTICS**

**OKOYE GLORIA .I.**

**Problem Statement**

 Food is one thing people cannot do without. And people of different culture have unique ways in which they prepare food, giving rise to what we fondly call “cuisine”. The restaurant business is one that caters to this supply. It affords people the ability to try out dishes from various places without having to visit such places. With emerging trends, restaurants are restricted to walk-in services but have diversified to offering delivery services.

 In Order to provide maximum services and remain relevant, said business takes into consideration; situation, customers choice in preferred cuisine, price range, mode of order, rating, as will be seen from the dataset.

The data sciance techniques that will will be employed over the course of this project looks to understand and analyze restuarant data by providing an insight into customers preferences in order to provide maximum satisfaction as well as boost business. This project will also employ the use of a number of machine learning models in order to make predictions to aid restaurants.

# **OBJECTIVES**

Analyze restaurant data to perform EDA, build predictive models, and derive actionable insights.

# **Week 1: Exploration**


1.   Dataset Overview

Explore dataset dimensions.

Check for missing values.

Perform data type conversions as needed.

2.   Target Variable

Analyze "Aggregate rating" distribution.

Address any class imbalances.

3.   Descriptive Analysis



Calculate statistics for numerical columns.

 Explore categorical variables.

 Identify top 5 cuisines and cities.
"""

#import libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

#data importation


url = "https://raw.githubusercontent.com/Oyeniran20/axia_class_cohort_7/refs/heads/main/Dataset%20.csv"
df = pd.read_csv(url)
df

"""# **Data Understanding/Overview**"""

df.shape

df.info()

#check for duplicate

df.duplicated().sum()

"""* After importation, the data to be worked with has a total of 9551 rows and 21 columns.
* The data type for each of the columns is in its right state. Hence, there was no need for data type conversion.
* There is no duplicated data also
* All columns have the appropraite data type as shown above

# **Check for Missing** **Data**
"""

#check for missing values

df.isna().sum().sort_values(ascending=False)

#Get percentage of missing data

(df.isna().sum().sort_values(ascending = False)) / len(df) * 100

"""Only the column,'Cuisines', contains misssing data. The value of missing data for this column is,"9", which is < 5% of the entire data.

Due to this %, missing data can be handled by dropping.
"""

#handling missing data

df=df.dropna()

#recheck missing values

df.isna().sum().sort_values(ascending=False)

"""# **Descriptive Analysis**"""

#calculating statistics for numerical columns


pd.set_option('display.float_format', '{:.2f}'.format)
df.describe().T

"""Statistical analysis is done for the 8 numerical columns;

1.   The minimum value for the Average Cost for two shows 0.00 which could mean certains meals for two were free or there was a mixup during data entry. The max value is 800000. This could be an outlier due to the large difference in the values of the 25th, 50th and 75th portion.
2.   The price range goes from 1(minimum) - 4(maximum). Our statistical analysis shows that most restaurants fall into the average price range of 2.
3.   The minimum aggregate rating shows 0.00 implying that there was no rating for some restaurants and a maximum of 4.90. No restaurant got a perfect 5 star. Most of the restaurants got a rating of 3-4 as seen in the 50%, 75% and max.
4. 0.00 minimum votes signify that a few restaurants had no review, the maximum votes at 10934.00 which is a wide gap from the 25th, 50th and 75th values. Likely prescence of an outlier.

# Exploration of categorical columns
"""

#Identification of top 5 cities and cuisines

City_count = df['City'].value_counts(ascending=False).head()

df['Cuisines']= df['Cuisines'].str.split(', ').explode().reset_index(drop=True)
Cuisine_count = df['Cuisines'].value_counts(ascending=False).head()


print('Top City:  ', City_count)
print()
print('Top Cuisine:  ', Cuisine_count)

plt.figure(figsize=(5,5))
sns.barplot(x=City_count.index, y=City_count.values)
plt.xlabel('City')
plt.title('Top 5 Most Common Cities')

plt.figure(figsize=(5,5))
sns.barplot(x=Cuisine_count.index, y=Cuisine_count.values)
plt.xlabel('Cuisine')
plt.title('Top 5 Most Common Cuisine')

"""# **Insights**

*   Most restaurants are located in India as seen by the top 5 cities being located there.
*   The city of New Delhi has the highest number of rstaurants-5473, followed by Gurgaon-1118, Noida-1080, Faridabad-251, Ghaziabad-25.
*   The "North Indian" cuisine appears to be more commonly served seeing as it has a total of 1749 resturants serving it.
*   The "Chinese" cuisine is next, having a total of 1184 resturants.
*   The "Fast Foods","Mughali" and "Italian" cuisines have 807, 441 and 427 restutarants respectively serving them.
*   The two largest cuisines that are served in these restaurants being the "North Indian" and "Chinese" can be attributed to the fact that a very good number of restaurants are located in Asia. It can be said that this cuisine is native to this area.



"""



"""# **Target Variable**"""

#getting info about target variable

df['Aggregate rating'].info()

#visualize target variable

sns.histplot(df['Aggregate rating'], bins=25, kde=True)
plt.xlabel('Aggregate rating')
plt.title('Distribution of Aggregate rating')

plt.figure(figsize=(5,5))
sns.boxplot(df['Aggregate rating'], color='red')
plt.xlabel('Aggregate rating')
plt.ylabel('Count')
plt.title('Boxplot of Aggregate rating')

plt.show()

"""*   Target variable is left skwed. This is seen by the extended distribution tail to the left of the chart.
*   This is seen from the chart,majority of the ratings are moderate,a good number of the ratings are high while few restuarants have low ratings.

**Data Transformation**
"""

df['log_Aggregate rating'] = np.log1p(df['Aggregate rating'])
sns.histplot(df['Aggregate rating'], kde=True, bins=20, color='navy')
plt.xlabel('Aggregate rating')
plt.title('Distribution of Aggregate rating')

plt.show()

"""* In an attempt to handle skweness, data transformation was carried out on our target column.
* Logarithm transformation was used.
* However, skweness was not handled and data not transformed. Therefore, original target column will be used

# **Week 2: Data Visualization**



1.  Visualization
*   Create histograms, bar plots, and box plots of ratings.
*   Compare average ratings across cuisines and cities.
2.  Geospatial Analysis
*   Map restaurant locations using coordinates.
*   Analyze distribution across cities.
*   Correlate location with ratings.
3.  Additional Analysis
*   Identify outliers and their effects.
*   Determine relationship between votes and ratings.

# **Visualization of Rating**
"""

#creating histogram and boxplot

plt.figure(figsize=(5,5))
sns.histplot(df['Aggregate rating'], kde=True, bins=20, color='red')
plt.xlabel('Aggregate rating')
plt.ylabel('Count')
plt.title('Histogram of Aggregate rating')

plt.figure(figsize=(5,5))
sns.boxplot(df['Aggregate rating'], color='red')
plt.xlabel('Aggregate rating')
plt.ylabel('Count')
plt.title('Boxplot of Aggregate rating')

plt.show()

"""*   The distribution is skwed.
*   The boxplot provides a summary of the aggregate rating distribution.
*   The median rating is close to 3.5, which aligns with the histogram.
*   There are outliers below 2, meaning a few restaurants received significantly lower ratings.
*   Most restaurants have an aggregate rating between 2.8 and 4.5.
*   The distribution suggests that high ratings are common, and very low ratings are rare

# **Comparison of Rating across Cities and Cuisines**
"""

#average rating across cities
#going forward aggregate rating is still being used and not the transformed one (log_aggregate) because skweness was not handled
Avg_ratings_for_cities=df.groupby('City')['Aggregate rating'].mean().head(15)
Avg_ratings_for_cities

#average rating across cuisines

Avg_ratings_for_cuisines=df.groupby('Cuisines')['Aggregate rating'].mean().sort_values(ascending=False).head(15)
Avg_ratings_for_cuisines

"""# **Visualization of Average Rating across City and Cuisine**"""

plt.figure(figsize=(5,5))
sns.barplot(x=Avg_ratings_for_cities.values, y=Avg_ratings_for_cities.index, color='orange')
plt.xlabel('Average rating')
plt.ylabel('City')
plt.title('Average Rating across Cities')

plt.figure(figsize=(5,5))
sns.barplot(x=Avg_ratings_for_cuisines.values, y=Avg_ratings_for_cuisines.index,  color='orange')
plt.xlabel('Average rating')
plt.ylabel('Cuisines')
plt.title('Average Rating across Cuisines')

plt.show()

"""1.   Average Rating Across Cities
* The average ratings across all listed cities fall between 3 - 4.3.
* Abu Dhabi and Bangalore have the highest ratings.
* Ratings are a testament of either standard of restaurant or customer experience. The difference between the ratings across these cities are not so great meaning there is a similarity and stability between standard of restaurant or customer experience in these cities.

2. Average Rating Across Cuisines
* The average ratings across all listed cities fall between 3 - 4.8.

# **Geospatial Analysis**
"""

map_df = df[['Restaurant Name', 'City','Address',	'Locality',	'Locality Verbose', 'Latitude', 'Longitude', 'Cuisines', 'Aggregate rating', 'Votes' ]]
map_df.head()

!pip install geopandas mapclassify
import geopandas as gpd
import folium
import mapclassify

my_map = gpd.GeoDataFrame(map_df, geometry=gpd.points_from_xy(map_df.Longitude, map_df.Latitude), crs="EPSG:4326")
my_map

my_map.to_file("output_shapefile.shp", driver="ESRI Shapefile")

MAP = my_map.explore(column="Aggregate rating", popup=["City", "Restaurant Name"])
MAP

"""* Points on the map signifies concentration of restaurants on said locations
* A good number of restaurnts are located in Asia, precisely India

# **Additional Analysis**


*   Identifying Outliers
"""

outlier_num_cols=['Aggregate rating', 'Average Cost for two', 'Votes']
outlier_num_cols

for col in outlier_num_cols:
    plt.figure(figsize=(12,6))
    plt.subplot(1,2,1)
    sns.boxplot(x=df[col], color='yellow')
    plt.title(f'{col} Boxplot')
    plt.xlabel(col)
    plt.xticks(rotation=45)


    plt.subplot(1, 2, 2)
    sns.histplot(df[col], kde = True, bins = 20, color='yellow')
    plt.title(f'{col} Distribution')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

"""**Effect of the Outliers**


*   The distribution of the histplot and the boxplot show outliers.
*   The boxplot of votes shows a large difference in value after 8000.
*   The prescence of extreme values after 500000 for average cost of two causes a heavy skweness.
*   The histogram of both that there is more distribution of restaurants at low values for average cost and votes.
*   This outliers may have had effect on the statistial analysis (mean,median...) done earlier
*   Outliers likely to affect model training and performance.

**Relationship Between Votes and Ratings**
"""

#Checking for correlation between votes and rating

#Checking for correlation between votes and rating

correlation = df['Votes'].corr(df['Aggregate rating'])

print(f"Correlation : {correlation:.2f}")

"""*  There is a positive correlation between votes and aggregate rating.
*  With a correlation value of 0.31, the correlation is a weak positive.
*

# **Week 3:Customer Preference**

1.  Cuisine Analysis
*   Identify highest-rated cuisines
2.  Price Range
*   Compare ratings across price points
3.  Service Features
*   Analyze table booking and delivery
*   Analyze relationships between cuisines and ratings.
*   Identify popular cuisines by votes.
*   Determine which price ranges receive highest ratings.
*   Compare restaurants with and without table booking.
4.  Table Booking Impact
*   Determine if table booking availability affects ratings across different cities
*   Compare average ratings with and without this feature.
5.  Customer Preferences
*   Online Delivery Analysis
*   Calculate percentage of restaurants offering delivery.
*   Analyze availability across different price ranges.
*   Identify specific cuisines that consistently receive higher ratings.
*   Determine city-specific preferences.
"""

df.head()

"""**Cuisine** **Analysis**"""

Highest_rated_cuisines=df.groupby('Cuisines')['Aggregate rating'].mean().sort_values(ascending=False).head(20)
#grouped by mean to get the average of the rating
Highest_rated_cuisines

plt.figure(figsize=(5,5))
sns.barplot(x=Highest_rated_cuisines.index, y=Highest_rated_cuisines.values, color='pink')
plt.xlabel('Cuisine')
plt.ylabel('Aggregate rating')
plt.xticks(rotation=90)
plt.title('Highest Rated Cuisines')

plt.show()

"""* Ratings of cuisines shows which the customers prefer
* The graph shows the 20 highest rated cuisines based on the average ratings

**Price Range**
"""

price_range_and_ratings=df.groupby('Price range')['Aggregate rating'].mean()
price_range_and_ratings

#comparing rating across price points

plt.figure(figsize=(5,5))
sns.barplot(x=price_range_and_ratings.index, y=price_range_and_ratings.values)
plt.xlabel('price_range_and_ratings')
plt.title('Relationship between pice and rating')

plt.show()

"""* Restaurants with low price range are rated low and vice versa
* This means that high priced restaurants are rated highly

# **Service Features**

## **Analysis of Table booking**
"""

#checking the number of restaurants that offer table booking and the number that does not.

df['Has Table booking'].value_counts()

"""* Table booking as a service rendered by restaurants seems to be widely adopted.
* 8393 restaurants out of the 9551 have table booking
* The remaining 1158 do not have table booking

**Visualization of Table booking**
"""

#representing the information above

sns.countplot(data=df, x='Has Table booking', color='orange', width=0.4 )
plt.xticks(rotation=45)
plt.show()

"""**Relationship between Table booking and Votes, Table booking and Rating**"""

#still on the analysis of table booking

Booking_per_votes=df.groupby('Has Table booking')['Votes'].mean()

Booking_per_rating=df.groupby('Has Table booking')['Aggregate rating'].mean()

print('Table booking   :',  Booking_per_votes)
print()
print('Table book  ',Booking_per_rating)

#visualizing the above information above

plt.figure(figsize=(10,6))

plt.subplot(1,2,1)
sns.barplot(x=Booking_per_votes.index, y=Booking_per_votes.values, color='orange')
plt.xlabel('Votes')
plt.ylabel('Table booking')
plt.title('Booking_per_votes')

plt.subplot(1,2,2)
sns.barplot(x=Booking_per_rating.index, y=Booking_per_rating.values, color='orange')
plt.xlabel('Average rating')
plt.ylabel('Table booking')
plt.title('Booking_per_rating')

plt.tight_layout()
plt.show()

"""*   Restaurants that offer table booking get more votes than those that do not.
*   The difference between the votes are large.
*   Restaurants that have table booking have an average vote of about 350
*   Restaurants that do not have table booking have an average vote of about 140
*   Restaurants that offer table booking were rated slightly higher than those that do not.
*   The difference between the two is seen to be minute. That is to say that this option did not necessarily affect the ratings.

**Analysis of Delivery**
"""

#checking the number of restaurants that have online delivery and the number that does not

df['Has Online delivery'].value_counts()

"""*  7091 restaurants do not have online delivery
*  The remaining 2451 have online delivery

**Visualization of Online Delivery**
"""

#representing the above information

sns.countplot(data=df, x='Has Online delivery', color='orange', width=0.4 )
plt.xticks(rotation=45)
plt.show()

"""**Relationship between Online Delivery and Votes, Online Delivery and Rating**"""

Delivery_per_votes=df.groupby('Has Online delivery')['Votes'].mean()

Delivery_per_rating=df.groupby('Has Online delivery')['Aggregate rating'].mean()

print('Online delivery:  ',Delivery_per_rating)
print()
print('Online delivery:  ',Delivery_per_votes)

#visualization

plt.figure(figsize=(10,6))

plt.subplot(1,2,1)
sns.barplot(x=Delivery_per_votes.index, y=Delivery_per_votes.values, color='orange')
plt.xlabel('Votes')
plt.ylabel('Online Delivery')
plt.title('Delivery_per_votes')

plt.subplot(1,2,2)
sns.barplot(x=Delivery_per_rating.index, y=Delivery_per_rating.values, color='orange')
plt.xlabel('Average rating')
plt.ylabel('Online Delivery')
plt.title('Delivery_per_rating')

plt.tight_layout()
plt.show()

"""1.  Online delivery boosts votes but not ratings.
* More people voted for restaurants with delivery services than they did for
those without.
* Average rating between those that offer delivery and those that do not is almost the same. The online delivery option has no implication on how restaurants are rated.

# **Customer Preferences**

### **Popular cuisines by votes**
"""

#to get popularity, cuisines are grouped by the mean of the votes(average votes)

cuisine_votes = df.groupby("Cuisines")["Votes"].mean().sort_values(ascending = False).head(15)
cuisine_votes

#visualization

plt.figure(figsize=(5,5))
sns.barplot(x=cuisine_votes.values, y=cuisine_votes.index,  color='Yellow')
plt.xlabel('Votes')
plt.ylabel('Cuisines')
plt.title('Popular Cuisines by Votes')

plt.show()

"""*   Cuisines presented shows a mixture of several meal preferences.
*   Most people get Pub food seeing as it has the highest number of votes at 10934.
*   New American comes in second with 1171.50 votes.
*   Indian, Goan and Portuguese has the least votes. Safe to say that the preference for them was low.

###**Percentage of restaurants offering delivery**
"""

#Total number of restaurants

Total_restaurant = df['Restaurant Name'].shape[0]
Total_restaurant

#Restaurants offering delivery

Restaurant_with_delivery = df[df['Has Online delivery']=='Yes'].shape[0]
Restaurant_with_delivery

#calculating percentage

Percentage = (Restaurant_with_delivery / Total_restaurant)*100

print('Percentage of restaurants offering delivery: ', f"{Percentage:.2f}")

"""*  Only 25.66% of the total restsurant offers delivery services.
*  That leaves 73.33% of restaurants that do not render delivery services.

# **Week 4:Predictive Modeling**

1.   Feature Engineering
*  Extract additional features from existing columns.Create new features by encoding categorical variables.
2. Model Building
* Build regression models to predict restaurant ratings.
* Split data into training and testing sets.
3. Model Evaluation
* Evaluate using RMSE, MAE, and R-squared. Compare different algorithms like linear regression and random forest.
"""

df.head(2)

#getting our numerical and categorical columns

num_cols = df.select_dtypes(include=['float64', 'int64']).columns
cat_cols = df.select_dtypes(include=['object']).columns

print (num_cols)
print (cat_cols)

# correlation matrix

corr_matrix = df[num_cols].corr()
corr_matrix

"""1. Restaurant ID
* weak negative correlation with targets variable,longitude, latitude, price range, votes
* ⁠no correlation with average cost for two.

2. Country code
* weak correlation with target variable, votes, price range, average cost for two, latitude, restaurant ID
* ⁠weak negative correlation with longitude

3. Longitude
* weak negative correlation with every variable but itself.

4. Latitude
* no correlation with target variable
* ⁠weak negative correlation with restaurant ID, average cost for two , price range and votes
* ⁠a weak positive correlation with country code, longitude.

5. Average cost for two
* weak positive correlation to target variable, country code, longitude, price range, votes
* ⁠no correlation to restaurant ID.

6. Price range
* weak negative correlation with restaurant ID, longitude,latitude,
* ⁠weak positive with country code, average cost for two,
* ⁠good positive correlation with aggregate rating and votes

7. Votes
* weak negative, which restaurant ID, longitude, latitude.
* ⁠ weak positive with, country code, average cost for two.
* ⁠ good positive correlation with price range, aggregate rating

8. Aggregate rating
* weak, negative with restaurant ID, longitude, latitude
* ⁠ weak positive with country code and average cost for two.
* ⁠ good positive correlation with price range and votes

# **Model Building**

**Visualization of Correlation Matrix**
"""

plt.figure(figsize=(10,10))
sns.heatmap(corr_matrix, annot=True, fmt=' .2f', cmap='Blues', center=0, linewidth=1, linecolor='black')
plt.title('Correlation Heatmap', fontsize=18, color='navy',fontweight='bold')

plt.show()

#drop columns with negative correlation to target variable and some categorical that are not necessary

df = df.drop(['Restaurant ID','Country Code','Address', 'Latitude', 'Longitude', 'Average Cost for two', 'Locality', 'Locality Verbose'],axis=1)

"""**Data Splitting**"""

#Separate in to Feature and target variable

X= df.drop(columns=['Aggregate rating', 'log_Aggregate rating'])
y = df['Aggregate rating']

from sklearn.model_selection import train_test_split

train_inputs, test_input, train_target, test_target=train_test_split(X, y, test_size=0.3, random_state=27)

"""##**Data Preprocessing**

**Encoding**
"""

#import libraries

from sklearn.preprocessing import OneHotEncoder, StandardScaler

encoder = OneHotEncoder(drop='first')

train_inputs.head(2)

cat_col=['City', 'Cuisines', 'Has Table booking',	'Has Online delivery',	'Is delivering now','Switch to order menu', 'Rating color', 'Rating text']
cat_col

encoder = OneHotEncoder(handle_unknown='ignore')
train_cat = encoder.fit_transform(train_inputs[cat_col])
test_cat = encoder.transform(test_input[cat_col])

"""**Scaling**"""

scaler = StandardScaler()

num_cols = [col for col in num_cols if col in train_inputs.columns]

train_num = scaler.fit_transform(train_inputs[num_cols])
test_num = scaler.transform(test_input[num_cols])

#Combining

train_processed = np.hstack((train_num,train_cat.toarray()))
test_processed = np.hstack((test_num,test_cat.toarray()))

train_processed

test_processed

"""# **Comparing Different Algorithms**

**Model Selection**
"""

from sklearn.linear_model import LinearRegression

model = LinearRegression()

"""**Model Training**"""

model.fit(train_processed,train_target)

train_prediction = model.predict(train_processed)
test_prediction = model.predict(test_processed)

"""**Model Evaluation**"""

#Importing the needed libraries

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

#Evaluating the model using Mean Absolute Error


train_MAE = mean_absolute_error(train_prediction,train_target)
test_MAE =mean_absolute_error(test_prediction,test_target)

train_MAE,test_MAE

"""1. Both MAE values are quite low, the model is making small errors in predictions.
2. The training and testing MAE values are very close, meaning the model generalizes well and does not overfit.
3. The model performs slightly better on the data it was trained on as can be seen by the slightly higher test error

**Model Performance**
"""

#Evaluating the model using r2_score

train_score = r2_score(train_prediction,train_target)
test_score =r2_score(test_prediction,test_target)

train_score,test_score

"""* The model fits the data well with minimal error
* No signs of overfitting.
* The model explains 98.67% of variance in training data and 98.61% in test data
"""

#Evaluating the model using the Root Mean Square Error


train_rmse = np.sqrt(mean_absolute_error(train_prediction,train_target))
test_rmse =np.sqrt(mean_absolute_error(test_prediction,test_target))

train_rmse,test_rmse

"""* The model is performing well, with high R² scores and low RMSE values.
* No overfitting
"""

train_target

train_prediction

error = train_target - train_prediction
error

prediction_table = pd.DataFrame({
    'Actual' : train_target,
    'Predicted' : train_prediction,
    'Error' : error
})

prediction_table.head(10)

"""* Most predictions are close to actual value
* This indicates that the model is making fairly accurate predictions
* The model is performing well, with errors generally being small
* For case of actual being 0.00 and predicted being -0.00, The model almost perfectly predicted the value
* Largest error observed is 0.36.

**Random Forest**
"""

from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor

(train_inputs.select_dtypes(include=['object']).columns)

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()

# Applying encoding to each categorical column in the train and test inputs
for col in train_inputs.select_dtypes(include=['object']).columns:
    train_inputs[col] = encoder.fit_transform(train_inputs[col])

for col in train_inputs.select_dtypes(include=['object']).columns:
    train_inputs[col] = encoder.fit_transform(train_inputs[col])

rf_model = RandomForestRegressor(n_estimators=100, random_state=27)
rf_model.fit(train_inputs, train_target)

RF_train_prediction = model.predict(train_processed)
RF_test_prediction = model.predict(test_processed)

# Calculating performance metrics for the Random Forest predictions

train_mae_2 = mean_absolute_error(train_target, RF_train_prediction)
test_mae_2 = mean_absolute_error(test_target, RF_test_prediction)
train_R2 = r2_score(train_target, RF_train_prediction)
test_R2 = r2_score(test_target, RF_test_prediction)
train_rmae_2 = np.sqrt(train_mae_2)
test_rmae_2 = np.sqrt(test_mae_2)

(train_mae_2,test_mae_2)

(train_rmae_2,test_rmae_2)

(train_R2,test_R2)

"""

*   Results are similar to that of the regression model

"""

